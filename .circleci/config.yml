version: 2.1

orbs: 
  kubernetes: circleci/kubernetes@1.3.0

commands:
  destroy-environment:
    description: Destroy frontend CloudFormation stack and resources given a workflow ID.
    parameters:
      WorkflowID:
        type: string
    steps:
      - run:
          name: Destroy environments
          when: on_fail
          command: |
            echo "Destroying environment: << parameters.WorkflowID >> "
            echo "Empty S3 bucket for static frontend website..."
            aws s3 rm "s3://website-<< parameters.WorkflowID >>" --recursive
            echo "Delete S3 bucket for static frontend website..."
            aws cloudformation delete-stack --stack-name capstone-website-<< parameters.WorkflowID >>

  revert-cluster:
    description: Delete K8s deployment and point service to old deployment.
    parameters:
      WorkflowID:
        type: string
    steps:
      - kubernetes/install-kubectl
      - run:
          name: Revert K8s resources
          when: on_fail
          command: |
            # Update kubectl to point to EKS cluster
            aws eks update-kubeconfig --name $CLUSTER_NAME
            kubectl delete -f ./backend/backend-deployment.yaml
            # TODO: point service to old K8s resources (old deployment)

jobs:
  lint-code:
    docker:
      - image: python:3.7.3-stretch
    steps:
      - checkout
      - run:
          name: Lint code
          command: |
            # a call of 'make install' would need 'npm' to be installed first which is not necessary for linting.
            pip install --upgrade pip && pip install -r backend/src/requirements.txt
            make install_hadolint
            make lint

  build-frontend:
    docker:
      - image: circleci/node:13.8.0
    steps:
      - checkout
      - restore_cache:
          keys: [frontend-build]
      - run:
          name: Build frontend
          command: |
            cd frontend
            npm install
            npm run build
      - save_cache:
          paths: [frontend/node_modules]
          key: frontend-build

  build-backend:
    docker:
      - image: docker
    steps:
      - checkout
      - setup_remote_docker:
          docker_layer_caching: true
      - run:
          name: Install bash
          command: |
            apk add bash
      - run:
          name: Build backend
          command: |
            docker build --tag udacity-project ./backend

  test-frontend:
    docker:
      - image: circleci/node:13.8.0
    steps:
      - checkout
      - restore_cache:
          keys: [frontend-build]
      - run:
          name: Test frontend
          command: |
            cd frontend
            npm install
            npm test -- a

  test-backend:
    machine:
      image: ubuntu-1604:202007-01 # Use machine executor instead of Docker executor to avoid a container-in-container environment for the backend test.
    steps:
      - checkout
      - run:
          name: Install dependencies
          command: |
            sudo apt-get update
            sudo apt-get upgrade -y

            # Install minikube
            curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
            sudo install minikube-linux-amd64 /usr/local/bin/minikube
            rm minikube-linux-amd64
      - kubernetes/install-kubectl
      - run:
          name: Set environment variables
          command: |
            echo DATABASE_USERNAME="$DATABASE_USERNAME" | sudo tee -a /etc/environment > /dev/null
            echo DATABASE_PASSWORD="$DATABASE_PASSWORD" | sudo tee -a /etc/environment > /dev/null
            echo DATABASE_NAME="$DATABASE_NAME" | sudo tee -a /etc/environment > /dev/null
            echo DATABASE_HOST="$DATABASE_HOST" | sudo tee -a /etc/environment > /dev/null
            echo DATABASE_PORT="$DATABASE_PORT" | sudo tee -a /etc/environment > /dev/null
      - run:
          name: Test backend in Docker container
          command: |
            docker build --tag udacity-project ./backend
            export CONTAINER_ID="$(docker run --rm -d -p 80:8000 --env-file /etc/environment --name udacity-app udacity-project)"
            echo "CONTAINER_ID: $CONTAINER_ID"

            export API_URL_DOCKER="localhost/api/wisdom"
            echo "API_URL_DOCKER: $API_URL_DOCKER"

            echo "Wait 20 s until Docker container is properly running..."
            sleep 20s

            if curl $API_URL_DOCKER | grep "categories"; then
              echo SUCCESS
            else
              echo FAIL
              exit 1
            fi

            if curl $API_URL_DOCKER | grep "image-urls"; then
              echo SUCCESS
            else
              echo FAIL
              exit 1
            fi

            if curl $API_URL_DOCKER | grep "source"; then
              echo SUCCESS
            else
              echo FAIL
              exit 1
            fi

            if curl $API_URL_DOCKER | grep "wisdom"; then
              echo SUCCESS
            else
              echo FAIL
              exit 1
            fi

            docker stop $CONTAINER_ID
      - run:
          name: Upload Docker image to repository
          command: |
            ./scripts/upload_docker.sh # TODO use current workflow id as tag (--> pass as parameter to script). upload initial tag with default uuid as tag id
      - run:
          name: Test backend in Minikube
          command: |
            ./scripts/run_kubernetes_minikube.sh

            export API_URL_K8S="$(minikube service $(minikube service list | grep -o -e "\S*capstone\S*") --url=true)/api/wisdom"
            echo "API_URL_K8S: $API_URL_K8S"

            if curl $API_URL_K8S | grep "categories"; then
              echo SUCCESS
            else
              echo FAIL
              exit 1
            fi

            if curl $API_URL_K8S | grep "image-urls"; then
              echo SUCCESS
            else
              echo FAIL
              exit 1
            fi

            if curl $API_URL_K8S | grep "source"; then
              echo SUCCESS
            else
              echo FAIL
              exit 1
            fi

            if curl $API_URL_K8S | grep "wisdom"; then
              echo SUCCESS
            else
              echo FAIL
              exit 1
            fi

            minikube stop

  deploy-infrastructure:
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - kubernetes/install-kubectl
      - run:
          name: Create S3 bucket for static frontend website
          command: |
            aws cloudformation deploy \
              --stack-name "capstone-website-${CIRCLE_WORKFLOW_ID:0:7}" \
              --template-file .circleci/files/frontend-bucket.yaml \
              --parameter-overrides WorkflowID="${CIRCLE_WORKFLOW_ID:0:7}" \
              --tags project=capstone
      - run:
          name: Fetch and save LoadBalancer URL of K8s service
          command: |
            aws eks update-kubeconfig --name $CLUSTER_NAME
            export LOADBALANCER_URL="http://$(kubectl get $(kubectl get svc -o name | grep -o -e ".*capstone.*") --output jsonpath='{.status.loadBalancer.ingress[0].hostname}'):8000"
            echo "LOADBALANCER_URL: $LOADBALANCER_URL"
            curl https://kvdb.io/$KVDB_BUCKET_ID/LoadBalancer_Url -d "$LOADBALANCER_URL" -u $KVDB_WRITE_KEY:$KVDB_WRITE_KEY
      - destroy-environment:
          WorkflowID: ${CIRCLE_WORKFLOW_ID:0:7}

  deploy-frontend:
    docker:
      - image: circleci/node:13.8.0
    steps:
      - checkout
      - restore_cache:
          keys: [frontend-build]
      - run:
          name: Install dependencies
          command: |
            sudo apt update
            sudo apt upgrade -y
            sudo apt install -y awscli
      - run:
          name: Deploy frontend website
          command: |
            export LOADBALANCER_URL="$(curl https://kvdb.io/$KVDB_BUCKET_ID/LoadBalancer_Url -u $KVDB_READ_KEY:$KVDB_READ_KEY)"
            echo "LOADBALANCER_URL: $LOADBALANCER_URL"
            echo REACT_APP_HOST="$LOADBALANCER_URL" > frontend/.env.production
            cat frontend/.env.production
            cd frontend
            npm install
            npm run build
            aws s3 cp build s3://website-${CIRCLE_WORKFLOW_ID:0:7} --recursive
      - destroy-environment:
          WorkflowID: ${CIRCLE_WORKFLOW_ID:0:7}

  deploy-backend:
    docker:
      - image: circleci/node:13.8.0
    steps:
      - checkout
      - run:
          name: Install dependencies
          command: |
            sudo apt update
            sudo apt upgrade -y
            sudo apt install -y awscli
      - run:
          name: Deploy backend
          command: |
            echo "Update deployment and service..."
            # Grep old id from backend-deployment via kubectl get deploy -o name | grep -o -e ".*capstone.*"
            # save old id to KVdb as old workflow id
            # update used container tag to current workflow id
            # change old id in service and deployment to new id --> new deployment; service gets updated to new deployment --> blue/green deployment
            # save cache to save changed service and backend yaml file (with new id)
            ./scripts/run_kubernetes_eks.sh
      - destroy-environment:
          WorkflowID: ${CIRCLE_WORKFLOW_ID:0:7}
      - revert-cluster:
          WorkflowID: ${CIRCLE_WORKFLOW_ID:0:7}

  smoke-test:
    docker:
      - image: circleci/node:13.8.0
    steps:
      - checkout
      - run:
          name: Install dependencies
          command: |
            sudo apt update
            sudo apt upgrade -y
            sudo apt install -y curl
            sudo apt install -y awscli
            aws --version
      - run:
          name: Backend smoke test.
          command: |
            export LOADBALANCER_URL="$(curl https://kvdb.io/$KVDB_BUCKET_ID/LoadBalancer_Url -u $KVDB_READ_KEY:$KVDB_READ_KEY)"
            export API_URL="$LOADBALANCER_URL/api/wisdom"
            echo "API_URL: $API_URL"

            if curl $API_URL | grep "categories"; then
              echo SUCCESS
            else
              echo FAIL
              exit 1
            fi

            if curl $API_URL | grep "image-urls"; then
              echo SUCCESS
            else
              echo FAIL
              exit 1
            fi

            if curl $API_URL | grep "source"; then
              echo SUCCESS
            else
              echo FAIL
              exit 1
            fi

            if curl $API_URL | grep "wisdom"; then
              echo SUCCESS
            else
              echo FAIL
              exit 1
            fi
      - run:
          name: Frontend smoke test.
          command: |
            export URL="http://website-${CIRCLE_WORKFLOW_ID:0:7}.s3-website.${AWS_DEFAULT_REGION}.amazonaws.com"
            echo "Frontend URL: $URL"

            if curl $URL | grep "GetWise"; then
              echo SUCCESS
              exit 0
            else
              echo FAIL
              exit 1
            fi
      - destroy-environment:
          WorkflowID: ${CIRCLE_WORKFLOW_ID:0:7}
      - revert-cluster:
          WorkflowID: ${CIRCLE_WORKFLOW_ID:0:7} # todo: attach cache from backend deployment to get service and deployment yaml with new id

#  cloudfront-update:
#    docker:
#      - image: amazon/aws-cli
#    steps:
#      - checkout
#      - run:
#          name: Install dependencies
#          command: |
#            yum update -y
#            yum install -y curl
#            aws --version
#      - run:
#          name: Store old WorkflowID
#          command: |
#            OldWorkflowID=$(aws cloudformation \
#              list-exports --query "Exports[?Name==\`WorkflowID\`].Value" \
#              --no-paginate --output text)
#            echo OldWorkflowID: $OldWorkflowID
#            curl -H "Content-Type: text/plain" -H "token: ${CIRCLE_WORKFLOW_ID}" --request PUT --data "$OldWorkflowID" https://api.memstash.io/values/OldWorkflowID
#      - run:
#          name: Update cloudfront distribution
#          command: |
#            # Change the initial stack name, as applicable to you
#            aws cloudformation deploy \
#              --template-file .circleci/files/cloudfront.yml \
#              --stack-name InitialStack \
#              --parameter-overrides WorkflowID="${CIRCLE_WORKFLOW_ID:0:7}" \
#              --tags project=udapeople
#      - destroy-environment:
#          WorkflowID: ${CIRCLE_WORKFLOW_ID:0:7}
#      - revert-cluster:
#          WorkflowID: ${CIRCLE_WORKFLOW_ID:0:7}
#
#  cleanup:
#    docker:
#      - image: amazon/aws-cli
#    steps:
#      - checkout
#      - run:
#          name: Get old stack workflow id
#          command: |
#            # Fetch the Old workflow ID
#            curl -H "token: ${CIRCLE_WORKFLOW_ID}" --request GET https://api.memstash.io/values/OldWorkflowID >> ~/OldWorkflowID.txt
#            echo OldWorkflowID: $(cat ~/OldWorkflowID.txt)
#            echo Current Workflow ID: ${CIRCLE_WORKFLOW_ID:0:7}
#            # Fetch the stack names
#            aws cloudformation list-stacks --query "StackSummaries[*].StackName" \
#              --stack-status-filter CREATE_COMPLETE --no-paginate --output text >> ~/stacks.txt
#            echo Stack names: $(cat ~/stacks.txt)
#      - run:
#          name: Remove old stacks and files
#          command: |
#            OldWorkflowID=$(cat ~/OldWorkflowID.txt)
#            Stacks=$(cat ~/stacks.txt)
#            echo OldWorkflowID: $OldWorkflowID
#            echo Stack names: ${Stacks[@]}
#            if [[ "${Stacks[@]}" =~ "${OldWorkflowID}" ]]
#            then
#              echo "Destroying environment: ${OldWorkflowID}"
#              if echo $Stacks | grep -q "udapeople-backend-${OldWorkflowID}"
#              then
#                aws cloudformation delete-stack --stack-name udapeople-backend-${OldWorkflowID}
#              fi
#              if echo $Stacks | grep -q "udapeople-frontend-${OldWorkflowID}"
#              then
#                aws s3 rm "s3://udapeople-${OldWorkflowID}" --recursive
#                aws cloudformation delete-stack --stack-name udapeople-frontend-${OldWorkflowID}
#              fi
#            else
#              echo "--------------- Cannot Cleanup ---------------"
#            fi
#
#
workflows:
  default:
    jobs:
      - lint-code
      - build-frontend:
          requires: [lint-code]
      - build-backend:
          requires: [lint-code]
      - test-frontend:
          requires: [build-frontend]
      - test-backend:
          requires: [build-backend]
      - deploy-infrastructure:
          requires: [test-frontend, test-backend]
          filters:
            branches:
              only: [main]
      - deploy-frontend:
          requires: [deploy-infrastructure]
      - deploy-backend:
          requires: [deploy-infrastructure]
      - smoke-test:
          requires: [deploy-backend, deploy-frontend]
#      - cloudfront-update:
#          requires: [smoke-test]
#      - cleanup:
#          requires: [cloudfront-update]
